{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for detecting if a patient electronic health record is a physical visit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the notebook\n",
    "- This jupyter notebook will take VetCompass data, format it for use within machine learning models, then use simple embedding techniques to train a model for classification.\n",
    "- A physical visit is any electronic health record in where a patient recieved a physical examination. \n",
    "- The embedding technique used is #\n",
    "- While multiple neural network architypes are available to solve this problem, this example uses a convulational neural network as an example. \n",
    "\n",
    "## About the author\n",
    "- This notebook was born from a MSc project which used this very data - link to it\n",
    "\n",
    "\n",
    "## Notes to reader\n",
    "- Explanatory instructions provided within cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set imports and relative data paths.\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Bidirectional\n",
    "import matplotlib.pyplot as plt   \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "nltk.download('stopwords');\n",
    "nltk.download('punkt')\n",
    "\n",
    "SEED = 2023\n",
    "\n",
    "\n",
    "DATA_DIR: str = \"/home/aaron/aaron_data/\"\n",
    "UPDATED_DATA_DIR: str = \"/home/aaron/updated_data\"\n",
    "RAW_DF: str = os.path.join(DATA_DIR, \"combined_documents.csv\")\n",
    "PATIENT_LEVEL_DF: str = os.path.join(\n",
    "    UPDATED_DATA_DIR, \"secondary_patient_level_annotations_v1.csv\"\n",
    ")\n",
    "PROBLEM_LEVEL_DF: str = os.path.join(\n",
    "    UPDATED_DATA_DIR, \"secondary_problem_level_annotations_v1.csv\"\n",
    ")\n",
    "REARRANGED_DATA_FILEPATH: str = os.path.join(\n",
    "    \"/home/aaron/timeseries_nlp/data/\", \"rearranged_data.csv\"\n",
    ")\n",
    "REARRANGED_DATA_FILEPATH_FLAT: str = os.path.join(\n",
    "    \"/home/aaron/timeseries_nlp/data/\", \"rearranged_data_flat.csv\"\n",
    ")\n",
    "\n",
    "class Config:\n",
    "    vocab_size = 20000\n",
    "    batch_size = 16\n",
    "    epochs = 50\n",
    "    labels = ['Visit']\n",
    "    cleaned_ehr_column = \"rejoined\"\n",
    "    stopword_ehr_column = \"stopwords_removed\"\n",
    "    tokenized_ehr_column = \"tokenized_ehr\"\n",
    "    target_column = \"Visit\"\n",
    "    ehr_column = \"ehr\"\n",
    "    max_len = 200\n",
    "    max_features = 5000\n",
    "    dimensions = 100\n",
    "config = Config()\n",
    "\n",
    "# Set seeds to ensure reproduceability\n",
    "\n",
    "tf.random.set_seed(\n",
    "    SEED\n",
    ")\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "# https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organise VC Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all three CSVs into a pandas data frame (I was provided with a combined_documents.csv, secondary_patient_level_annotations.csv and a secondary_problem_level_annotations.csv)\n",
    "for i in tqdm(range(0, 3), ncols=100, desc=\"Loading data..\"):\n",
    "    raw_df_as_read = pd.read_csv(RAW_DF)\n",
    "    patient_level_annotations_as_read = pd.read_csv(PATIENT_LEVEL_DF)\n",
    "    problem_level_annotations_as_read = pd.read_csv(PROBLEM_LEVEL_DF)\n",
    "print(\"------Loading is completed ------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are given lots of extraeneous information, so lets strip non relavent data. Note that patient ID is unique to each patient. \n",
    "# Firstly we create a copy of the data, create an additional column called Date which contains the date record was made\n",
    "# Then we drop unnessicary columns, and filter all records by the study date (1/1/2019 -> 31/12/2019)\n",
    "\n",
    "raw_df = raw_df_as_read.copy()\n",
    "\n",
    "\"Split the recorded date to DateTimeDay column, so we can get records existing on a single day\"\n",
    "raw_df[[\"DateTimeDay\", \"DateTimeSeconds\"]] = raw_df[\"RecordedDate\"].str.split(\n",
    "    \"T\", expand=True\n",
    ")\n",
    "raw_df[\"Date\"] = pd.to_datetime(raw_df[\"DateTimeDay\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "raw_df.drop(['Type', 'DateTimeSeconds', 'CaseNumber', 'DataSiloName', 'LatestPatientVersionID'], axis=1, inplace=True, errors='ignore') #Drop un-needed columns\n",
    "raw_df = raw_df[(raw_df[\"Date\"] >= \"2019-1-1\") & (raw_df[\"Date\"] <= \"2019-12-31\")] #Only include patients withint study start date\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_level_annotations = patient_level_annotations_as_read.copy()\n",
    "\n",
    "patient_level_annotations.drop([\n",
    " 'CaseNumber',\n",
    " 'LatestPatientVersionID',\n",
    " 'BreedVeNomID',\n",
    " 'DataSiloName',\n",
    " 'VetCompassBreed',\n",
    " 'SpeciesVeNomID',\n",
    " 'SourceSpecies',\n",
    " 'VetCompassSpecies',\n",
    " 'FirstNoteDate',\n",
    " 'LatestPatientVersionDate',\n",
    " 'FirstVersionDate',\n",
    " 'LastNoteDate',\n",
    " 'CodingStarted',\n",
    " 'FirstClinicId',\n",
    " 'SourceClinicName',\n",
    " 'IsArchived'], axis=1, inplace=True)\n",
    "patient_level_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_patients = set(patient_level_annotations.loc[patient_level_annotations['Is this patient included in the study_1'] == 'Yes', 'PatientID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_level_annotations = problem_level_annotations_as_read.copy()\n",
    "\n",
    "# Drop all patients NOT in study\n",
    "problem_level_annotations  =  problem_level_annotations[problem_level_annotations.PatientID.isin(study_patients)]\n",
    "\n",
    "problem_level_annotations[[\"DateTimeDay\", \"DateTimeSeconds\"]] = problem_level_annotations[\"DocumentDate\"].str.split(\n",
    "    \" \", expand=True\n",
    ")\n",
    "problem_level_annotations[\"Date\"] = pd.to_datetime(problem_level_annotations[\"DateTimeDay\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "problem_level_annotations.drop([ 'DataSiloName', 'ProblemID', 'VeNomID',\n",
    "       'VetCompassProblemID', 'TermName', 'Context',\n",
    "       'TextHighlightedWhenCreated','CodingStarted',\n",
    "       'CaseNumber', 'LatestPatientVersionID', 'BreedVeNomID',\n",
    "       'VetCompassBreed', 'SpeciesVeNomID', 'SourceSpecies',\n",
    "       'VetCompassSpecies', 'FirstVersionDate', 'LatestPatientVersionDate',\n",
    "       'FirstNoteDate', 'LastNoteDate', 'FirstClinicId', 'SourceClinicName',\"DateTimeDay\", \"DateTimeSeconds\",\"DocumentDate\",\n",
    " 'IsArchived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multiple_entries(df)->list:\n",
    "    \"Check there are not multiple entries per day, if so remove these days from the data pool \"\n",
    "    multiple_entries_list = []\n",
    "    for i in set(df.PatientID.tolist()):\n",
    "        results = df.loc[df.PatientID ==i]\n",
    "        if not results.Date.is_unique:\n",
    "            multiple_entries_list.append(i)\n",
    "    return multiple_entries_list\n",
    "\n",
    "df = problem_level_annotations[~problem_level_annotations.PatientID.isin(check_multiple_entries(problem_level_annotations))]\n",
    "\n",
    "if not len(check_multiple_entries(df)) == 0:\n",
    "    raise Exception(\"Sorry, but it appears you have multiple categorised days!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"You have {len(df.PatientID.unique())} patients included in this study\")\n",
    "print(f\"and a total of {len(df.index)} 24 hour periods classified\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the EHRs to the DF\n",
    "ehrs = []\n",
    "for index, row in df.iterrows():\n",
    "    ehr = raw_df[(raw_df.PatientId ==row.PatientID) & (raw_df.Date==row.Date)]\n",
    "    ehr_list = ehr.Document.to_list()\n",
    "    s = ' '.join(ehr_list)\n",
    "    ehrs.append(s)\n",
    "    \n",
    "df.loc[:,'ehr'] = ehrs.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some prettying of the dataframe\n",
    "df = df.replace({'Is this note a visit_2': {'Yes': 1, 'No': 0}})\n",
    "df.rename(columns={'Is this note a visit_2': 'Visit'}, inplace=True)\n",
    "\n",
    "# Rules based breed disambugation. \n",
    "\n",
    "# df['SourceBreed'].replace('TERRIER - WEST HIGHLAND W','West Highland White Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('SPANIEL - CAVALIER KING C','Cavalier King Charles Spaniel',inplace=True)\n",
    "# df['SourceBreed'].replace('King Charles Spaniel','Cavalier King Charles Spaniel',inplace=True)\n",
    "# df['SourceBreed'].replace('English Cocker Spaniel','Spaniel (Cocker)',inplace=True)\n",
    "# df['SourceBreed'].replace('German Shepherd','German Shepherd Dog',inplace=True)\n",
    "# df['SourceBreed'].replace('SHEPHERD - GERMAN UNSPEC','German Shepherd Dog',inplace=True)\n",
    "# df['SourceBreed'].replace('SHEPHERD - GERMAN OLD','German Shepherd Dog',inplace=True)\n",
    "# df['SourceBreed'].replace('SHEPHERD DOG - GERMAN','German Shepherd Dog',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua - Longhaired','Chihuahua (Long Coat)',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua','Chihuahua (Smooth Coat)',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua Smooth Coat','Chihuahua (Smooth Coat)',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua, Short-Haired','Chihuahua (Smooth Coat)',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua, Long-Haired','Chihuahua (Long Coat)',inplace=True)\n",
    "# df['SourceBreed'].replace('Wire Fox Terrier','Fox Terrier (Wire)',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - FOX (UNSPECIFIE','Fox Terrier (Smooth)',inplace=True)\n",
    "# df['SourceBreed'].replace('Husky','Siberian Husky',inplace=True)\n",
    "# df['SourceBreed'].replace('Husky - Siberian','Siberian Husky',inplace=True)\n",
    "# df['SourceBreed'].replace('Doberman Pinscher','Dobermann',inplace=True)\n",
    "# df['SourceBreed'].replace('Chinese Shar-Pei','Shar Pei',inplace=True)\n",
    "# df['SourceBreed'].replace('English Springer Spaniel','Spaniel (English Springer)',inplace=True)\n",
    "# df['SourceBreed'].replace('Springer Spaniel','Spaniel (English Springer)',inplace=True)\n",
    "# df['SourceBreed'].replace('Mastiff','Mastiff',inplace=True)\n",
    "# df['SourceBreed'].replace('Poodle Standard','Poodle (Standard)',inplace=True)\n",
    "# df['SourceBreed'].replace('Collie','Border Collie',inplace=True)\n",
    "# df['SourceBreed'].replace('Collie - Border','Border Collie',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Miniature           ','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Miniature Smooth Haired','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Miniature Long Haired','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Miniature (Smooth-Haired)','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('DACHSHUND, MINI LONG-HAIR','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('DACHSHUND, STAND WIRE-H','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('DACHSHUND, MINI SMOOTH-H','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Standard','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Dachshund Smooth Haired','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Longhaired Miniature Dachshund','Dachshund',inplace=True)\n",
    "# df['SourceBreed'].replace('Golden Retriever','Retriever (Golden)',inplace=True)\n",
    "# df['SourceBreed'].replace('RETRIEVER - GOLDEN (GOLDE','Retriever (Golden)',inplace=True)\n",
    "# df['SourceBreed'].replace('RETRIEVER - LABRADOR (LAB','Retriever (Labrador)',inplace=True)\n",
    "# df['SourceBreed'].replace('Retriever - Labrador','Retriever (Labrador)',inplace=True)\n",
    "# df['SourceBreed'].replace('Retriever','Retriever (Labrador)',inplace=True)\n",
    "# df['SourceBreed'].replace('JAPANESE  AKITA','Japanese Akita Inu',inplace=True)\n",
    "# df['SourceBreed'].replace('Akita ','Japanese Akita Inu',inplace=True)\n",
    "# df['SourceBreed'].replace('American Akita','Japanese Akita Inu',inplace=True)\n",
    "# df['SourceBreed'].replace('Akita - Japanese Inu','Japanese Akita Inu',inplace=True)\n",
    "# df['SourceBreed'].replace('Shiba Inu','Japanese Shiba Inu',inplace=True)\n",
    "# df['SourceBreed'].replace('Weimeraner','Weimaraner',inplace=True)\n",
    "# df['SourceBreed'].replace('Shih-tzu','Shih Tzu',inplace=True)\n",
    "# df['SourceBreed'].replace('Miniature Poodle','Poodle (Miniature)',inplace=True)\n",
    "# df['SourceBreed'].replace('Chinese Crested, Hairless','Chinese Crested',inplace=True)\n",
    "# df['SourceBreed'].replace('Poodle, Toy','Poodle (Miniature)',inplace=True)\n",
    "# df['SourceBreed'].replace('Cocker Spaniel','Spaniel (Cocker)',inplace=True)\n",
    "# df['SourceBreed'].replace('SPANIEL - COCKER (UNSPECI','Spaniel (Cocker)',inplace=True)\n",
    "# df['SourceBreed'].replace('Spaniel - Cocker, working','Spaniel (Cocker)',inplace=True)\n",
    "# df['SourceBreed'].replace('SPANIEL - SPRINGER, ENGLI','Spaniel (English Springer)',inplace=True)\n",
    "# df['SourceBreed'].replace('SPANIEL - SPRINGER (UNSPE','Spaniel (English Springer)',inplace=True)\n",
    "# df['SourceBreed'].replace('ENGLISH SPANIEL','Spaniel (English Springer)',inplace=True)\n",
    "# df['SourceBreed'].replace('STAFFORDSHIRE BULL TERRIE','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - STAFF BULL ENG','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - AMERICAN PIT BU','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - STAFF BULL UNSP','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - STAFFORDSHIRE BULL','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Staffordshire Bull, English','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('Irish Staffordshire Bull Terrier','Staffordshire Bull Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('Olde English Bulldogge','Bulldog',inplace=True)\n",
    "# df['SourceBreed'].replace('SCHNAUZER, MINIATURE (MIN','Minature Schanuzer',inplace=True)\n",
    "# df['SourceBreed'].replace('SCHNAUZER, MINIATURE (MIN','Minature Schanuzer',inplace=True)\n",
    "# df['SourceBreed'].replace('Setter - Irish, Red','Irish Setter',inplace=True)\n",
    "# df['SourceBreed'].replace('SETTER - IRISH, RED AND W','Irish Setter',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - YORKSHIRE (YORK','Yorkshire Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - YORKSHIRE, MINI','Yorkshire Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Yorkshire','Yorkshire Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - JACK RUSSELL','Jack Russell Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('American Pit Bull Terrier','Bull Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Tibetan','Tibetan Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Tibetan','Tibetan Terrior',inplace=True)\n",
    "# df['SourceBreed'].replace('Bulldog - British','Bulldog',inplace=True)\n",
    "# df['SourceBreed'].replace('Collie - Scottish Smooth','Collie (Smooth)',inplace=True)\n",
    "# df['SourceBreed'].replace('Collie - Scottish Rough','Collie (Rough)',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - BORDER (BORDER','Border Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('POINTER - GERMAN, ROUGH-H','German Wirehaired Pointer',inplace=True)\n",
    "# df['SourceBreed'].replace('Setter - English','English Setter',inplace=True)\n",
    "# df['SourceBreed'].replace('HUNGARIAN VIZSLA, SMOOTH-','Hungarian Visla',inplace=True)\n",
    "# df['SourceBreed'].replace('Ridgeback - Rhodesian','Rohdesian Ridgeback',inplace=True)\n",
    "# df['SourceBreed'].replace('English Bull Terrier','Bull Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Poodle Toy','Poodle (Toy)',inplace=True)\n",
    "# df['SourceBreed'].replace('Bulldog - French','French Bulldog',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Bull American','Bull Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('HEELER - LANCASHIRE (HEEL','Lancashire Heeler',inplace=True)\n",
    "# df['SourceBreed'].replace('Bichon - Frise','Bichon Frise',inplace=True)\n",
    "# df['SourceBreed'].replace('bichon','Bichon Frise',inplace=True)\n",
    "# df['SourceBreed'].replace('Sheepdog - Old English','Old English Sheepdog',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Boston','Boston Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Patterdale','Patterdale Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Welse','Welsh Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Italian Spinone (Spinoni)','Italian Spinone',inplace=True)\n",
    "# df['SourceBreed'].replace('SHEEPDOG - SHETLAND (SHEL','Shetland Sheepdog',inplace=True)\n",
    "# df['SourceBreed'].replace('Basset','Basset Hound',inplace=True)\n",
    "# df['SourceBreed'].replace('Schnauzer, Giant','Giant Schnauzer',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Scottish','Scottish Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Hound - Basset','Basset Hound',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier - Lakeland','Lakeland Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('POINTER - GERMAN, SHORT-H','German Shorthaired Pointed',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - JACK RUSS. MINI','Jack Russel Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Schnauzer, Miniature','Minature Schnauzer',inplace=True)\n",
    "# df['SourceBreed'].replace('TERRIER - STAFFORDSHIRE,','Staffordshire Terrier',inplace=True)\n",
    "# df['SourceBreed'].replace('Beagle - English','Beagle',inplace=True)\n",
    "# df['SourceBreed'].replace('Toy Poodle','Poodle (Toy)',inplace=True)\n",
    "# df['SourceBreed'].replace('Nova Scotia Duck Tolling Retriever ','Retriever (Nova Scotia Duck Tolling)',inplace=True)\n",
    "\n",
    "# df['SourceBreed'].replace('English Cocker Spaniel X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Pedenco Maneto','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Jug','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Plummer Terrier','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Shikoku','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('CHINA JACK','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cavachon','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cavapoo','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cockerpoo','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Huntaway','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Sprocker','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Labrador Retriever X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Shih Tzu X ','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Jack Russell Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Collie X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Golden Retriever X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('English Springer Spaniel X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Maltese X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Border Collie X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Bichon Frise X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Northern Inuit Dog X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Rottweiler X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Yorkshire Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Schnauzer X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Chihuahua X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('CROSSBREED - SMALL','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Lhasa Apso X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Irish Wolfhound X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('German Shepherd X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cavalier King Charles Spaniel X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Pug X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Boxer X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Rhodesian Ridgeback X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Lurcher X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Staffie X German Shepherd','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('X breed sprocker','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cross Breed Small','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('jack russellxchihuahua','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Staffordshire Bull Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Border Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Husky X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('BORDER COLLIE CROSS DACHSHUND','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cross Breed Medium','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Springerdor (springer x lab)','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Jack Russell Cross','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Australian labradoodle','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('crossbreed small','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Mixed','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Doberman X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('crossbreed - medium','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Labradoodle - medium','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Basset Hound X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Poodle x Schnauzer','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('French Bulldog X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Pomeranian X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Akita X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Bulldog X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Miniature Schnauzer X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('German Pointer X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Beagle X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('MIX BREED','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Plummer Terrier X','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('SHIH-TZU CROSS','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Cross Breed','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Yorkshire Terrier cross Poodle','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Terrier Cross','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Crossbreed - Large','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('SHIH-TZU CROSS','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Patterdale Cross','Crossbreed',inplace=True)\n",
    "# df['SourceBreed'].replace('Chorkie','Crossbreed',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list, i.e. [Donald, Trump, tweets]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(tokenized_column):\n",
    "    \"\"\"Return a list of tokens with English stopwords removed. \n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column of tokenized data from tokenize()\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list with stopwords removed.\n",
    "\n",
    "    \"\"\"\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    return [word for word in tokenized_column if not word in stops]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(tokenized_column):\n",
    "    \"\"\"Rejoins a tokenized word list into a single string. \n",
    "    \n",
    "    Args:\n",
    "        tokenized_column (list): Tokenized column of words. \n",
    "        \n",
    "    Returns:\n",
    "        string: Single string of untokenized words. \n",
    "    \"\"\"\n",
    "    return ( \" \".join(tokenized_column))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[config.tokenized_ehr_column] = df.apply(lambda x: tokenize(x[config.ehr_column]), axis=1)\n",
    "df[config.stopword_ehr_column ] = df.apply(lambda x: remove_stopwords(x[config.tokenized_ehr_column]), axis=1)\n",
    "df[config.cleaned_ehr_column] = df.apply(lambda x: rejoin_words(x[config.stopword_ehr_column]), axis=1)\n",
    "df[config.cleaned_ehr_column] = df[config.cleaned_ehr_column].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_split(df:pd.DataFrame, split_ratio: float):\n",
    "    \"\"\"\n",
    "    This function generates the two splits from an input dataframe, based on a ratio\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: pandas dataframe\n",
    "        split_ratio: float between 0.0 and 1.0 and represent the proportion of the dataset split\n",
    "    \n",
    "    Returns:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset      \n",
    "    \"\"\"\n",
    "       \n",
    "    text = df[config.cleaned_ehr_column].values.tolist()\n",
    "    targets = df[config.target_column].values.tolist()\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    rng.shuffle(text)\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    rng.shuffle(targets)\n",
    "\n",
    "    num_validation_samples = int(split_ratio * len(text))\n",
    "\n",
    "    train_samples = text[:-num_validation_samples]\n",
    "    val_samples = text[-num_validation_samples:]\n",
    "    train_labels = targets[:-num_validation_samples]\n",
    "    val_labels = targets[-num_validation_samples:]\n",
    "\n",
    "    \n",
    "    return train_samples, val_samples, train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[config.target_column].notna()]\n",
    "train_samples, val_samples, train_labels, val_labels = df_split(df, 0.2)\n",
    "val_samples, test_samples, val_labels, test_labels = df_split( pd.DataFrame(\n",
    "    {config.cleaned_ehr_column: val_samples,\n",
    "     config.target_column: val_labels\n",
    "    }), 0.3)\n",
    "\n",
    "print(f\"Total size of the dataset: {df.shape[0]}.\")\n",
    "print(f\"Training dataset: {len(train_samples)}.\")\n",
    "print(f\"Validation dataset: {len(val_samples)}.\")\n",
    "print(f\"Test dataset: {len(test_samples)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(train_samples, val_samples, embeddings_index):\n",
    "    \"\"\"\n",
    "    This function computes the embedding matrix that will be used in the embedding layer\n",
    "    \n",
    "    Parameters:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        embeddings_index: Python dictionary with word embeddings\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: embedding matrix with the dimensions (num_tokens, embedding_dim), where num_tokens is the vocabulary of the input data, and emdebbing_dim is the number of components in the GloVe vectors (can be 50,100,200,300)\n",
    "        vectorizer: TextVectorization layer      \n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = tf.keras.layers.TextVectorization(max_tokens=30000, output_sequence_length=config.max_len)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "    vectorizer.adapt(text_ds)\n",
    "    \n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "      \n",
    "    num_tokens = len(voc)\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "#   creating an embedding matrix\n",
    "    embedding_dim = len(embeddings_index['the'])\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "            \n",
    "    print(f\"Converted {hits} words ({misses} misses).\")\n",
    "\n",
    "    return embedding_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = '../timeseries_nlp/data/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(path_to_glove_file, 'r', encoding='utf8')\n",
    "for line in f:\n",
    "    splitLine = line.split(' ')\n",
    "    word = splitLine[0]                                  # the first entry is the word\n",
    "    coefs = np.asarray(splitLine[1:], dtype='float32')   # these are the vectors representing word embeddings\n",
    "    embeddings_index[word] = coefs\n",
    "print(\"Glove data loaded! In total:\",len(embeddings_index),\" words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_lstm_nn(embedding_matrix):  \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "    ) \n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.LSTM(128,return_sequences=True)(x)\n",
    "    x = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cnn_nn(embedding_matrix):  \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "    ) \n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Bidirectional\n",
    "def initialize_bilstm_nn(embedding_matrix):  \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "    ) \n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Bidirectional(layers.LSTM(128,return_sequences=True))(x)\n",
    "    x = layers.Conv1D(128, 3, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bilstm_stacked_nn(embedding_matrix):  \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False\n",
    "    ) \n",
    "    int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = Bidirectional(layers.LSTM(128,return_sequences=True))(x)\n",
    "    x = Bidirectional(layers.LSTM(128))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "def train_nn(model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop = False, verbose=1):\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=Adam(learning_rate=1e-3), \n",
    "              metrics=[\"acc\"])\n",
    "    \n",
    "    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "    \n",
    "    y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "    y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n",
    "    \n",
    "    if stop:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        history = model.fit(x_train, y_train, batch_size=config.batch_size, epochs=config.epochs, validation_data=(x_val, y_val), callbacks=[early_stopping], verbose=verbose)\n",
    "    else:\n",
    "        history = model.fit(x_train, y_train, batch_size=config.batch_size, epochs=config.epochs, validation_data=(x_val, y_val), verbose=verbose)\n",
    "        \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models: list = []\n",
    "models.append([initialize_cnn_nn(embedding_matrix), 'CNN'])\n",
    "models.append([initialize_bilstm_stacked_nn(embedding_matrix), 'BILSTM_STACKED'])\n",
    "models.append([initialize_bilstm_nn(embedding_matrix), 'BILSTM'])\n",
    "models.append([initialize_lstm_nn(embedding_matrix), 'LSTM'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories: list = []\n",
    "for model in models: \n",
    "    print(f\"Training model: {model[1]}\")\n",
    "    print()\n",
    "    trained_model, history = train_nn(model[0], train_samples, val_samples, train_labels, val_labels, vectorizer, stop=True)\n",
    "    print()\n",
    "    print(f\"{model[1]} trained for {len(history.history['loss'])} epochs\")\n",
    "    print()\n",
    "    histories.append([history, model[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a function to plot the history of Keras model training\n",
    "def plot_history(history, model_name:str):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax1 = fig.add_subplot(121)    \n",
    "    ax1.plot(x, acc, 'b', label='Training acc')\n",
    "    ax1.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    ax1.set_title(f'Training and validation accuracy for {model_name}')\n",
    "    ax1.set_ylim(0,1)\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(x, loss, 'b', label='Training loss')\n",
    "    ax2.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    ax2.set_title(f'Training and validation loss {model_name}')\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in histories:\n",
    "    plot_history(history[0], history[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(df, model):\n",
    "       \n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    preds = model(x)\n",
    "    end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "    probabilities = end_to_end_model.predict(df[config.cleaned_ehr_column])\n",
    "    \n",
    "    predictions = [1 if i > 0.5 else 0 for i in probabilities]\n",
    "    return predictions\n",
    "\n",
    "df1 = pd.DataFrame(\n",
    "    {config.cleaned_ehr_column: test_samples,\n",
    "     config.target_column: test_labels\n",
    "    })\n",
    "    \n",
    "for model in models:\n",
    "    predictions = predict_nn(df1, model[0])\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "    m.update_state(test_labels, predictions)\n",
    "    print(f'{model[1]} Test Model Accuracy: {m.result().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9006886ed9af46a1631a39384ef7a28ca1cf375f950361012be59baffb3e715d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
